You can execute our model natively on your mac. Here we included detailed guidelines to facilitate implementation.

### Step 1

Download xxx.bin(model link) 

### Step 2

Download a `.dmg` containing the latest version [ðŸ‘‰ here ðŸ‘ˆ](https://llamachat.app/api/download).

### Step 3

Configuration Model

choose Alpaca

<img width="604" alt="æˆªå±2023-04-24 20 10 52" src="https://user-images.githubusercontent.com/80691681/234009956-504948fb-2e69-43ad-b47f-ef7976282cd6.png">


Name the model, choose an avatar, and select the appropriate format from the Format dropdown list: GGML format (.bin extension). 

Put the model you just downloaded and upload it.

Set the model size to 7B

<img width="597" alt="æˆªå±2023-04-24 20 12 00" src="https://user-images.githubusercontent.com/80691681/234010143-5c0bf916-0383-44f8-b489-60c5aa2c52f1.png">


## Step 4

chat!

<img width="894" alt="æˆªå±2023-04-24 20 59 03" src="https://user-images.githubusercontent.com/80691681/234010299-379d3a05-f322-4bbe-b1b8-273d55830044.png">


### Note

1 The quantization of the local model is not very good, and it is necessary to set better parameters in order to generate valid results, because a lower precision is needed to run locally.

2 Each query affects each other, so in order to improve the effect, you need to click the clear button after each query.

<img width="688" alt="æˆªå±2023-04-24 20 39 03" src="https://user-images.githubusercontent.com/80691681/234010373-586b92af-768e-4224-a81e-b291de8c8d2b.png">

